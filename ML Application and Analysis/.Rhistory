# Load required libraries
library(dplyr)
library(tidyr)
library(readr)
# NOTE: Set to your working directory
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
print(paste("Current Working Directory: ",getwd()), sep="\n")
# Load the dataset
dataset <- read_csv("ground_weather_merged.csv")
# rename Image Present to Image
# Check the current column names in the dataframe
column_names <- names(dataset)
column_names
# Rename the "Image Present" column to "Image"
new_column_names <- ifelse(column_names == "Image Present", "Image", column_names)
# Update the dataframe with the new column names
names(dataset) <- new_column_names
names(dataset)
# Data preprocessing
# Remove any unnecessary columns
df <- select(dataset, -"Datetime",
-"precp",
-"Wind",
-"wspd",
-"dewPt",
#-"Image Present",
-"Low/Mid Clouds (%)",
-"High Cirrus (%)",
-"Long-lived Contrails (Count)",
-"Contrail-Cirrus (Count)",
-"Day Low/Mid",
-"Day Count Cirrs",
-"Day Count Cont LL",
-"Day Count Cont-Cirrus")
# Handle missing values
df <- na.omit(df)
# Split the dataset into predictors (X) and the target variable (y)
target_variable_column <- "Image"
X <- select(df, -target_variable_column)
y <- select(df, target_variable_column)
dim(X)[1] == dim(y)[1]
# Perform any additional data preprocessing steps as needed
# For example, scaling/standardizing variables, creating dummy variables, etc.
# Split the dataset into training and testing sets
set.seed(123)  # Set a random seed for reproducibility
train_indices <- sample(nrow(df), nrow(df) * 0.7)  # 70% for training
train_data <- X[train_indices, ]
test_data <- y[-train_indices, ]
#------------------------
model <- glm(Image ~ ., data = df, family = "binomial")
# Make predictions on the test data
predictions <- predict(model, newdata = df, type = "response")
# Evaluate the model (example: using accuracy)
predicted_classes <- ifelse(predictions > 0.5, 1, 0)
accuracy <- sum(predicted_classes == df$Image) / length(df$Image)
cat("Accuracy:", accuracy)
#------------------------
# Create a confusion matrix below
#------------------------
# Install the "caret" package if it's not already installed
# install.packages("caret")
# Load the required libraries
library(caret)
# Make predictions on the test data
predictions <- predict(model, newdata = df, type = "response")
# Convert predicted_classes and df$Image to factors with the same levels
predicted_classes <- factor(ifelse(predictions > 0.5, 1, 0))
df$Image <- factor(df$Image)
# Create the confusion matrix
confusion_matrix <- confusionMatrix(predicted_classes, df$Image)
# Print the confusion matrix
confusion_matrix
#-------------------------------------------------------------------------------
# VISUALIZATIONS
#-------------------------------------------------------------------------------
# Default Visual
library(pROC)
roc_obj <- roc(df$Image, predictions)
plot(roc_obj, main = "Receiver Operating Characteristic (ROC) Curve")
pr_obj <- roc(df$Image, predictions, direction = "<")
plot(pr_obj, main = "Precision-Recall Curve")
# Error: Unable to load package rms on my linux box
install.packages("rms")
library(rms)
calibration_plot <- calibration(predictions, df$Image, method = "bins")
plot(calibration_plot, main = "Calibration Plot")
#-------------------------------------------------------------------------------
# ggPlot  Visual
library(pROC)
library(ggplot2)
# ROC Curve
roc_obj <- roc(df$Image, predictions)
roc_data <- data.frame(
"FPR" = roc_obj$specificities,
"TPR" = roc_obj$sensitivities
)
roc_plot <- ggplot(roc_data, aes(x = FPR, y = TPR)) +
geom_line() +
geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
labs(x = "False Positive Rate", y = "True Positive Rate") +
ggtitle("Receiver Operating Characteristic (ROC) Curve")
roc_plot
#========================================
# Precision Recall Visualization
# Precision: measures the accuracy of positive predictions made by a model
# Precision = True Positives / (True Positives + False Positives)
# Recall: measures ability of a model to identify positive instances correctly
# Recall = True Positives / (True Positives + False Negatives)
#========================================
pr_obj <- roc(df$Image, predictions, direction = "<")
# Calculate Precision
pr_data <- data.frame(
"Recall" = pr_obj$sensitivities,
"Specificity" = pr_obj$specificities
)
pr_data$Precision <- with(pr_data, Recall / (Recall + (1 - Specificity)))
# Filter out rows with missing values
pr_data <- pr_data[complete.cases(pr_data), ]
pr_plot <- ggplot(pr_data, aes(x = Recall, y = Precision)) +
geom_line() +
labs(x = "Recall", y = "Precision") +
ggtitle("Precision-Recall Curve")
pr_plot
# Heatmap of Original Dataset w/targeted attributes
library(ggplot2)
# Calculate correlation matrix
numeric_df <- df[, sapply(df, is.numeric)]
cor_matrix <- cor(numeric_df)
# Convert correlation matrix to long format
cor_data <- reshape2::melt(cor_matrix)
# Create correlation heatmap using ggplot2
heatmap_plot <- ggplot(cor_data, aes(Var1, Var2, fill = value)) +
geom_tile() +
scale_fill_gradient(low = "white", high = "red") +
labs(x = "Variable 1", y = "Variable 2", title = "Correlation Heatmap") +
geom_text(aes(label = round(value, 2)), color = "black", size = 3)
heatmap_plot
#===============================================================================
# Display Outputs
#===============================================================================
heatmap_plot
confusion_matrix
roc_plot
pr_plot
